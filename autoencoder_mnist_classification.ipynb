{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 숫자 분류를 위한 Autoencoder+Softmax 분류기 예제 \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-0eea96f49cf9>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터를 다운로드 합니다.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# 학습에 필요한 설정값들을 정의합니다.\n",
    "learning_rate_RMSProp = 0.02\n",
    "learning_rate_GradientDescent = 0.5\n",
    "num_epochs = 100         # 반복횟수\n",
    "batch_size = 256          \n",
    "display_step = 1         # 몇 Step마다 log를 출력할지 결정합니다.\n",
    "input_size = 784         # MNIST 데이터 input (이미지 크기: 28*28)\n",
    "hidden1_size = 128       # 첫번째 히든레이어의 노드 개수 \n",
    "hidden2_size = 64        # 두번째 히든레이어의 노드 개수 \n",
    "\n",
    "# 입력을 받기 위한 플레이스홀더를 정의합니다.\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_size])   # 인풋을 위한 플레이스홀더를 정의합니다.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])           # True MNIST 숫자값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder 구조를 정의합니다.\n",
    "def build_autoencoder(x):\n",
    "    # 인코딩(Encoding) - 784 -> 128 -> 64\n",
    "    Wh_1 = tf.Variable(tf.random_normal([input_size, hidden1_size]))   \n",
    "    bh_1 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    H1_output = tf.nn.sigmoid(tf.matmul(x, Wh_1) +bh_1)\n",
    "    \n",
    "    Wh_2 = tf.Variable(tf.random_normal([hidden1_size, hidden2_size]))\n",
    "    bh_2 = tf.Variable(tf.random_normal([hidden2_size]))\n",
    "    H2_output = tf.nn.sigmoid(tf.matmul(H1_output, Wh_2) +bh_2)\n",
    "    \n",
    "    \n",
    "    # 디코딩(Decoding) 64 -> 128 -> 784\n",
    "    Wh_3 = tf.Variable(tf.random_normal([hidden2_size, hidden1_size]))\n",
    "    bh_3 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    H3_output = tf.nn.sigmoid(tf.matmul(H2_output, Wh_3) +bh_3)\n",
    "    Wo = tf.Variable(tf.random_normal([hidden1_size, input_size]))\n",
    "    bo = tf.Variable(tf.random_normal([input_size]))\n",
    "    X_reconstructed = tf.nn.sigmoid(tf.matmul(H3_output,Wo) + bo)\n",
    "    \n",
    "    return X_reconstructed, H2_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 분류기를 정의합니다.\n",
    "def build_softmax_classifier(x):\n",
    "    # 원본 MNIST 이미지(784) 대신 오토인코더의 압축된 특징(64)을 입력값으로 받습니다.\n",
    "    W_softmax = tf.Variable(tf.zeros([hidden2_size, 10]))    \n",
    "    b_softmax = tf.Variable(tf.zeros([10]))\n",
    "    y_pred = tf.nn.softmax(tf.matmul(x, W_softmax) + b_softmax)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Autoencoder를 선언합니다.\n",
    "# Autoencoder의 Reconstruction 결과(784), 압축된 Features(64)\n",
    "y_pred, extracted_features = build_autoencoder(x) \n",
    "\n",
    "# 타겟데이터는 인풋데이터와 같습니다.\n",
    "y_true = x\n",
    "\n",
    "# Softmax 분류기를 선언합니다. (입력으로 Autoencoder의 압축된 특징을 넣습니다.)\n",
    "y_pred_softmax = build_softmax_classifier(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-Training : MNIST 데이터 재구축을 목적으로하는 손실함수와 \n",
    "# 옵티마이저를 정의합니다.\n",
    "pretraining_loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # MSE 손실 함수\n",
    "opt1 = tf.train.RMSPropOptimizer(learning_rate_RMSProp)\n",
    "pretraining_train_step = opt1.minimize(pretraining_loss)\n",
    "\n",
    "\n",
    "# 2. Fine-Tuning :  MNIST 데이터 분류를 목적으로하는 손실함수와 옵티마이저를 \n",
    "# 정의합니다.\n",
    "# cross-entropy loss 함수\n",
    "#finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred_softmax), reduction_indices=[1]))     \n",
    "finetuning_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_softmax, labels=y))\n",
    "opt2 = tf.train.GradientDescentOptimizer(learning_rate_GradientDescent)\n",
    "finetuning_train_step = opt2.minimize(finetuning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 1, Pre-Training 손실 함수(pretraining_loss): 0.160760\n",
      "반복(Epoch): 2, Pre-Training 손실 함수(pretraining_loss): 0.107597\n",
      "반복(Epoch): 3, Pre-Training 손실 함수(pretraining_loss): 0.092069\n",
      "반복(Epoch): 4, Pre-Training 손실 함수(pretraining_loss): 0.082102\n",
      "반복(Epoch): 5, Pre-Training 손실 함수(pretraining_loss): 0.072761\n",
      "반복(Epoch): 6, Pre-Training 손실 함수(pretraining_loss): 0.065290\n",
      "반복(Epoch): 7, Pre-Training 손실 함수(pretraining_loss): 0.065066\n",
      "반복(Epoch): 8, Pre-Training 손실 함수(pretraining_loss): 0.064540\n",
      "반복(Epoch): 9, Pre-Training 손실 함수(pretraining_loss): 0.060754\n",
      "반복(Epoch): 10, Pre-Training 손실 함수(pretraining_loss): 0.057568\n",
      "반복(Epoch): 11, Pre-Training 손실 함수(pretraining_loss): 0.056943\n",
      "반복(Epoch): 12, Pre-Training 손실 함수(pretraining_loss): 0.058015\n",
      "반복(Epoch): 13, Pre-Training 손실 함수(pretraining_loss): 0.055342\n",
      "반복(Epoch): 14, Pre-Training 손실 함수(pretraining_loss): 0.052498\n",
      "반복(Epoch): 15, Pre-Training 손실 함수(pretraining_loss): 0.054082\n",
      "반복(Epoch): 16, Pre-Training 손실 함수(pretraining_loss): 0.050168\n",
      "반복(Epoch): 17, Pre-Training 손실 함수(pretraining_loss): 0.051023\n",
      "반복(Epoch): 18, Pre-Training 손실 함수(pretraining_loss): 0.048276\n",
      "반복(Epoch): 19, Pre-Training 손실 함수(pretraining_loss): 0.048549\n",
      "반복(Epoch): 20, Pre-Training 손실 함수(pretraining_loss): 0.049013\n",
      "반복(Epoch): 21, Pre-Training 손실 함수(pretraining_loss): 0.047002\n",
      "반복(Epoch): 22, Pre-Training 손실 함수(pretraining_loss): 0.044648\n",
      "반복(Epoch): 23, Pre-Training 손실 함수(pretraining_loss): 0.043359\n",
      "반복(Epoch): 24, Pre-Training 손실 함수(pretraining_loss): 0.040910\n",
      "반복(Epoch): 25, Pre-Training 손실 함수(pretraining_loss): 0.042095\n",
      "반복(Epoch): 26, Pre-Training 손실 함수(pretraining_loss): 0.036957\n",
      "반복(Epoch): 27, Pre-Training 손실 함수(pretraining_loss): 0.039100\n",
      "반복(Epoch): 28, Pre-Training 손실 함수(pretraining_loss): 0.036619\n",
      "반복(Epoch): 29, Pre-Training 손실 함수(pretraining_loss): 0.036119\n",
      "반복(Epoch): 30, Pre-Training 손실 함수(pretraining_loss): 0.036662\n",
      "반복(Epoch): 31, Pre-Training 손실 함수(pretraining_loss): 0.034370\n",
      "반복(Epoch): 32, Pre-Training 손실 함수(pretraining_loss): 0.034886\n",
      "반복(Epoch): 33, Pre-Training 손실 함수(pretraining_loss): 0.035860\n",
      "반복(Epoch): 34, Pre-Training 손실 함수(pretraining_loss): 0.034340\n",
      "반복(Epoch): 35, Pre-Training 손실 함수(pretraining_loss): 0.033589\n",
      "반복(Epoch): 36, Pre-Training 손실 함수(pretraining_loss): 0.034648\n",
      "반복(Epoch): 37, Pre-Training 손실 함수(pretraining_loss): 0.031459\n",
      "반복(Epoch): 38, Pre-Training 손실 함수(pretraining_loss): 0.031926\n",
      "반복(Epoch): 39, Pre-Training 손실 함수(pretraining_loss): 0.030131\n",
      "반복(Epoch): 40, Pre-Training 손실 함수(pretraining_loss): 0.026453\n",
      "반복(Epoch): 41, Pre-Training 손실 함수(pretraining_loss): 0.028755\n",
      "반복(Epoch): 42, Pre-Training 손실 함수(pretraining_loss): 0.028670\n",
      "반복(Epoch): 43, Pre-Training 손실 함수(pretraining_loss): 0.029235\n",
      "반복(Epoch): 44, Pre-Training 손실 함수(pretraining_loss): 0.026154\n",
      "반복(Epoch): 45, Pre-Training 손실 함수(pretraining_loss): 0.026257\n",
      "반복(Epoch): 46, Pre-Training 손실 함수(pretraining_loss): 0.027479\n",
      "반복(Epoch): 47, Pre-Training 손실 함수(pretraining_loss): 0.025504\n",
      "반복(Epoch): 48, Pre-Training 손실 함수(pretraining_loss): 0.024815\n",
      "반복(Epoch): 49, Pre-Training 손실 함수(pretraining_loss): 0.023886\n",
      "반복(Epoch): 50, Pre-Training 손실 함수(pretraining_loss): 0.023611\n",
      "반복(Epoch): 51, Pre-Training 손실 함수(pretraining_loss): 0.024288\n",
      "반복(Epoch): 52, Pre-Training 손실 함수(pretraining_loss): 0.023874\n",
      "반복(Epoch): 53, Pre-Training 손실 함수(pretraining_loss): 0.024669\n",
      "반복(Epoch): 54, Pre-Training 손실 함수(pretraining_loss): 0.023278\n",
      "반복(Epoch): 55, Pre-Training 손실 함수(pretraining_loss): 0.023465\n",
      "반복(Epoch): 56, Pre-Training 손실 함수(pretraining_loss): 0.022698\n",
      "반복(Epoch): 57, Pre-Training 손실 함수(pretraining_loss): 0.023164\n",
      "반복(Epoch): 58, Pre-Training 손실 함수(pretraining_loss): 0.023840\n",
      "반복(Epoch): 59, Pre-Training 손실 함수(pretraining_loss): 0.021116\n",
      "반복(Epoch): 60, Pre-Training 손실 함수(pretraining_loss): 0.023214\n",
      "반복(Epoch): 61, Pre-Training 손실 함수(pretraining_loss): 0.022698\n",
      "반복(Epoch): 62, Pre-Training 손실 함수(pretraining_loss): 0.022481\n",
      "반복(Epoch): 63, Pre-Training 손실 함수(pretraining_loss): 0.022010\n",
      "반복(Epoch): 64, Pre-Training 손실 함수(pretraining_loss): 0.021164\n",
      "반복(Epoch): 65, Pre-Training 손실 함수(pretraining_loss): 0.021381\n",
      "반복(Epoch): 66, Pre-Training 손실 함수(pretraining_loss): 0.020933\n",
      "반복(Epoch): 67, Pre-Training 손실 함수(pretraining_loss): 0.021072\n",
      "반복(Epoch): 68, Pre-Training 손실 함수(pretraining_loss): 0.021300\n",
      "반복(Epoch): 69, Pre-Training 손실 함수(pretraining_loss): 0.020713\n",
      "반복(Epoch): 70, Pre-Training 손실 함수(pretraining_loss): 0.021182\n",
      "반복(Epoch): 71, Pre-Training 손실 함수(pretraining_loss): 0.020566\n",
      "반복(Epoch): 72, Pre-Training 손실 함수(pretraining_loss): 0.022381\n",
      "반복(Epoch): 73, Pre-Training 손실 함수(pretraining_loss): 0.021743\n",
      "반복(Epoch): 74, Pre-Training 손실 함수(pretraining_loss): 0.021178\n",
      "반복(Epoch): 75, Pre-Training 손실 함수(pretraining_loss): 0.020252\n",
      "반복(Epoch): 76, Pre-Training 손실 함수(pretraining_loss): 0.021486\n",
      "반복(Epoch): 77, Pre-Training 손실 함수(pretraining_loss): 0.020331\n",
      "반복(Epoch): 78, Pre-Training 손실 함수(pretraining_loss): 0.019824\n",
      "반복(Epoch): 79, Pre-Training 손실 함수(pretraining_loss): 0.020547\n",
      "반복(Epoch): 80, Pre-Training 손실 함수(pretraining_loss): 0.018343\n",
      "반복(Epoch): 81, Pre-Training 손실 함수(pretraining_loss): 0.017013\n",
      "반복(Epoch): 82, Pre-Training 손실 함수(pretraining_loss): 0.018052\n",
      "반복(Epoch): 83, Pre-Training 손실 함수(pretraining_loss): 0.018830\n",
      "반복(Epoch): 84, Pre-Training 손실 함수(pretraining_loss): 0.017420\n",
      "반복(Epoch): 85, Pre-Training 손실 함수(pretraining_loss): 0.018296\n",
      "반복(Epoch): 86, Pre-Training 손실 함수(pretraining_loss): 0.019474\n",
      "반복(Epoch): 87, Pre-Training 손실 함수(pretraining_loss): 0.018249\n",
      "반복(Epoch): 88, Pre-Training 손실 함수(pretraining_loss): 0.017274\n",
      "반복(Epoch): 89, Pre-Training 손실 함수(pretraining_loss): 0.017478\n",
      "반복(Epoch): 90, Pre-Training 손실 함수(pretraining_loss): 0.018256\n",
      "반복(Epoch): 91, Pre-Training 손실 함수(pretraining_loss): 0.018366\n",
      "반복(Epoch): 92, Pre-Training 손실 함수(pretraining_loss): 0.018133\n",
      "반복(Epoch): 93, Pre-Training 손실 함수(pretraining_loss): 0.017840\n",
      "반복(Epoch): 94, Pre-Training 손실 함수(pretraining_loss): 0.015433\n",
      "반복(Epoch): 95, Pre-Training 손실 함수(pretraining_loss): 0.016644\n",
      "반복(Epoch): 96, Pre-Training 손실 함수(pretraining_loss): 0.017443\n",
      "반복(Epoch): 97, Pre-Training 손실 함수(pretraining_loss): 0.016400\n",
      "반복(Epoch): 98, Pre-Training 손실 함수(pretraining_loss): 0.016875\n",
      "반복(Epoch): 99, Pre-Training 손실 함수(pretraining_loss): 0.015923\n",
      "반복(Epoch): 100, Pre-Training 손실 함수(pretraining_loss): 0.015454\n",
      "Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료(Pre-Training)\n",
      "반복(Epoch): 1, Fine-tuning 손실 함수(finetuning_loss): 1.940686\n",
      "반복(Epoch): 2, Fine-tuning 손실 함수(finetuning_loss): 1.752298\n",
      "반복(Epoch): 3, Fine-tuning 손실 함수(finetuning_loss): 1.717440\n",
      "반복(Epoch): 4, Fine-tuning 손실 함수(finetuning_loss): 1.707103\n",
      "반복(Epoch): 5, Fine-tuning 손실 함수(finetuning_loss): 1.669022\n",
      "반복(Epoch): 6, Fine-tuning 손실 함수(finetuning_loss): 1.647609\n",
      "반복(Epoch): 7, Fine-tuning 손실 함수(finetuning_loss): 1.672404\n",
      "반복(Epoch): 8, Fine-tuning 손실 함수(finetuning_loss): 1.631688\n",
      "반복(Epoch): 9, Fine-tuning 손실 함수(finetuning_loss): 1.641478\n",
      "반복(Epoch): 10, Fine-tuning 손실 함수(finetuning_loss): 1.640887\n",
      "반복(Epoch): 11, Fine-tuning 손실 함수(finetuning_loss): 1.703102\n",
      "반복(Epoch): 12, Fine-tuning 손실 함수(finetuning_loss): 1.654779\n",
      "반복(Epoch): 13, Fine-tuning 손실 함수(finetuning_loss): 1.612922\n",
      "반복(Epoch): 14, Fine-tuning 손실 함수(finetuning_loss): 1.630422\n",
      "반복(Epoch): 15, Fine-tuning 손실 함수(finetuning_loss): 1.567183\n",
      "반복(Epoch): 16, Fine-tuning 손실 함수(finetuning_loss): 1.597096\n",
      "반복(Epoch): 17, Fine-tuning 손실 함수(finetuning_loss): 1.598905\n",
      "반복(Epoch): 18, Fine-tuning 손실 함수(finetuning_loss): 1.609649\n",
      "반복(Epoch): 19, Fine-tuning 손실 함수(finetuning_loss): 1.557875\n",
      "반복(Epoch): 20, Fine-tuning 손실 함수(finetuning_loss): 1.547927\n",
      "반복(Epoch): 21, Fine-tuning 손실 함수(finetuning_loss): 1.562270\n",
      "반복(Epoch): 22, Fine-tuning 손실 함수(finetuning_loss): 1.580613\n",
      "반복(Epoch): 23, Fine-tuning 손실 함수(finetuning_loss): 1.564880\n",
      "반복(Epoch): 24, Fine-tuning 손실 함수(finetuning_loss): 1.535782\n",
      "반복(Epoch): 25, Fine-tuning 손실 함수(finetuning_loss): 1.564809\n",
      "반복(Epoch): 26, Fine-tuning 손실 함수(finetuning_loss): 1.540930\n",
      "반복(Epoch): 27, Fine-tuning 손실 함수(finetuning_loss): 1.529079\n",
      "반복(Epoch): 28, Fine-tuning 손실 함수(finetuning_loss): 1.535338\n",
      "반복(Epoch): 29, Fine-tuning 손실 함수(finetuning_loss): 1.537738\n",
      "반복(Epoch): 30, Fine-tuning 손실 함수(finetuning_loss): 1.572757\n",
      "반복(Epoch): 31, Fine-tuning 손실 함수(finetuning_loss): 1.536883\n",
      "반복(Epoch): 32, Fine-tuning 손실 함수(finetuning_loss): 1.548338\n",
      "반복(Epoch): 33, Fine-tuning 손실 함수(finetuning_loss): 1.565803\n",
      "반복(Epoch): 34, Fine-tuning 손실 함수(finetuning_loss): 1.554342\n",
      "반복(Epoch): 35, Fine-tuning 손실 함수(finetuning_loss): 1.532584\n",
      "반복(Epoch): 36, Fine-tuning 손실 함수(finetuning_loss): 1.525443\n",
      "반복(Epoch): 37, Fine-tuning 손실 함수(finetuning_loss): 1.533615\n",
      "반복(Epoch): 38, Fine-tuning 손실 함수(finetuning_loss): 1.516866\n",
      "반복(Epoch): 39, Fine-tuning 손실 함수(finetuning_loss): 1.541477\n",
      "반복(Epoch): 40, Fine-tuning 손실 함수(finetuning_loss): 1.535225\n",
      "반복(Epoch): 41, Fine-tuning 손실 함수(finetuning_loss): 1.555887\n",
      "반복(Epoch): 42, Fine-tuning 손실 함수(finetuning_loss): 1.526350\n",
      "반복(Epoch): 43, Fine-tuning 손실 함수(finetuning_loss): 1.542625\n",
      "반복(Epoch): 44, Fine-tuning 손실 함수(finetuning_loss): 1.524475\n",
      "반복(Epoch): 45, Fine-tuning 손실 함수(finetuning_loss): 1.518431\n",
      "반복(Epoch): 46, Fine-tuning 손실 함수(finetuning_loss): 1.532403\n",
      "반복(Epoch): 47, Fine-tuning 손실 함수(finetuning_loss): 1.531214\n",
      "반복(Epoch): 48, Fine-tuning 손실 함수(finetuning_loss): 1.519429\n",
      "반복(Epoch): 49, Fine-tuning 손실 함수(finetuning_loss): 1.498655\n",
      "반복(Epoch): 50, Fine-tuning 손실 함수(finetuning_loss): 1.526524\n",
      "반복(Epoch): 51, Fine-tuning 손실 함수(finetuning_loss): 1.550233\n",
      "반복(Epoch): 52, Fine-tuning 손실 함수(finetuning_loss): 1.528408\n",
      "반복(Epoch): 53, Fine-tuning 손실 함수(finetuning_loss): 1.509640\n",
      "반복(Epoch): 54, Fine-tuning 손실 함수(finetuning_loss): 1.536713\n",
      "반복(Epoch): 55, Fine-tuning 손실 함수(finetuning_loss): 1.502258\n",
      "반복(Epoch): 56, Fine-tuning 손실 함수(finetuning_loss): 1.504177\n",
      "반복(Epoch): 57, Fine-tuning 손실 함수(finetuning_loss): 1.518040\n",
      "반복(Epoch): 58, Fine-tuning 손실 함수(finetuning_loss): 1.524558\n",
      "반복(Epoch): 59, Fine-tuning 손실 함수(finetuning_loss): 1.525326\n",
      "반복(Epoch): 60, Fine-tuning 손실 함수(finetuning_loss): 1.505000\n",
      "반복(Epoch): 61, Fine-tuning 손실 함수(finetuning_loss): 1.538325\n",
      "반복(Epoch): 62, Fine-tuning 손실 함수(finetuning_loss): 1.509641\n",
      "반복(Epoch): 63, Fine-tuning 손실 함수(finetuning_loss): 1.505380\n",
      "반복(Epoch): 64, Fine-tuning 손실 함수(finetuning_loss): 1.512213\n",
      "반복(Epoch): 65, Fine-tuning 손실 함수(finetuning_loss): 1.516489\n",
      "반복(Epoch): 66, Fine-tuning 손실 함수(finetuning_loss): 1.512501\n",
      "반복(Epoch): 67, Fine-tuning 손실 함수(finetuning_loss): 1.489528\n",
      "반복(Epoch): 68, Fine-tuning 손실 함수(finetuning_loss): 1.524365\n",
      "반복(Epoch): 69, Fine-tuning 손실 함수(finetuning_loss): 1.512701\n",
      "반복(Epoch): 70, Fine-tuning 손실 함수(finetuning_loss): 1.509448\n",
      "반복(Epoch): 71, Fine-tuning 손실 함수(finetuning_loss): 1.533289\n",
      "반복(Epoch): 72, Fine-tuning 손실 함수(finetuning_loss): 1.510982\n",
      "반복(Epoch): 73, Fine-tuning 손실 함수(finetuning_loss): 1.543078\n",
      "반복(Epoch): 74, Fine-tuning 손실 함수(finetuning_loss): 1.517628\n",
      "반복(Epoch): 75, Fine-tuning 손실 함수(finetuning_loss): 1.511786\n",
      "반복(Epoch): 76, Fine-tuning 손실 함수(finetuning_loss): 1.503081\n",
      "반복(Epoch): 77, Fine-tuning 손실 함수(finetuning_loss): 1.517859\n",
      "반복(Epoch): 78, Fine-tuning 손실 함수(finetuning_loss): 1.516014\n",
      "반복(Epoch): 79, Fine-tuning 손실 함수(finetuning_loss): 1.510983\n",
      "반복(Epoch): 80, Fine-tuning 손실 함수(finetuning_loss): 1.522470\n",
      "반복(Epoch): 81, Fine-tuning 손실 함수(finetuning_loss): 1.505647\n",
      "반복(Epoch): 82, Fine-tuning 손실 함수(finetuning_loss): 1.498115\n",
      "반복(Epoch): 83, Fine-tuning 손실 함수(finetuning_loss): 1.508990\n",
      "반복(Epoch): 84, Fine-tuning 손실 함수(finetuning_loss): 1.506499\n",
      "반복(Epoch): 85, Fine-tuning 손실 함수(finetuning_loss): 1.513988\n",
      "반복(Epoch): 86, Fine-tuning 손실 함수(finetuning_loss): 1.509081\n",
      "반복(Epoch): 87, Fine-tuning 손실 함수(finetuning_loss): 1.512074\n",
      "반복(Epoch): 88, Fine-tuning 손실 함수(finetuning_loss): 1.503289\n",
      "반복(Epoch): 89, Fine-tuning 손실 함수(finetuning_loss): 1.500152\n",
      "반복(Epoch): 90, Fine-tuning 손실 함수(finetuning_loss): 1.515235\n",
      "반복(Epoch): 91, Fine-tuning 손실 함수(finetuning_loss): 1.501788\n",
      "반복(Epoch): 92, Fine-tuning 손실 함수(finetuning_loss): 1.524442\n",
      "반복(Epoch): 93, Fine-tuning 손실 함수(finetuning_loss): 1.490562\n",
      "반복(Epoch): 94, Fine-tuning 손실 함수(finetuning_loss): 1.508709\n",
      "반복(Epoch): 95, Fine-tuning 손실 함수(finetuning_loss): 1.486040\n",
      "반복(Epoch): 96, Fine-tuning 손실 함수(finetuning_loss): 1.520523\n",
      "반복(Epoch): 97, Fine-tuning 손실 함수(finetuning_loss): 1.511086\n",
      "반복(Epoch): 98, Fine-tuning 손실 함수(finetuning_loss): 1.482398\n",
      "반복(Epoch): 99, Fine-tuning 손실 함수(finetuning_loss): 1.508180\n",
      "반복(Epoch): 100, Fine-tuning 손실 함수(finetuning_loss): 1.499387\n",
      "반복(Epoch): 101, Fine-tuning 손실 함수(finetuning_loss): 1.505802\n",
      "반복(Epoch): 102, Fine-tuning 손실 함수(finetuning_loss): 1.504047\n",
      "반복(Epoch): 103, Fine-tuning 손실 함수(finetuning_loss): 1.513579\n",
      "반복(Epoch): 104, Fine-tuning 손실 함수(finetuning_loss): 1.494730\n",
      "반복(Epoch): 105, Fine-tuning 손실 함수(finetuning_loss): 1.508292\n",
      "반복(Epoch): 106, Fine-tuning 손실 함수(finetuning_loss): 1.505399\n",
      "반복(Epoch): 107, Fine-tuning 손실 함수(finetuning_loss): 1.518596\n",
      "반복(Epoch): 108, Fine-tuning 손실 함수(finetuning_loss): 1.516619\n",
      "반복(Epoch): 109, Fine-tuning 손실 함수(finetuning_loss): 1.498216\n",
      "반복(Epoch): 110, Fine-tuning 손실 함수(finetuning_loss): 1.507297\n",
      "반복(Epoch): 111, Fine-tuning 손실 함수(finetuning_loss): 1.503326\n",
      "반복(Epoch): 112, Fine-tuning 손실 함수(finetuning_loss): 1.502158\n",
      "반복(Epoch): 113, Fine-tuning 손실 함수(finetuning_loss): 1.492524\n",
      "반복(Epoch): 114, Fine-tuning 손실 함수(finetuning_loss): 1.497124\n",
      "반복(Epoch): 115, Fine-tuning 손실 함수(finetuning_loss): 1.492660\n",
      "반복(Epoch): 116, Fine-tuning 손실 함수(finetuning_loss): 1.506015\n",
      "반복(Epoch): 117, Fine-tuning 손실 함수(finetuning_loss): 1.488744\n",
      "반복(Epoch): 118, Fine-tuning 손실 함수(finetuning_loss): 1.494232\n",
      "반복(Epoch): 119, Fine-tuning 손실 함수(finetuning_loss): 1.506527\n",
      "반복(Epoch): 120, Fine-tuning 손실 함수(finetuning_loss): 1.488232\n",
      "반복(Epoch): 121, Fine-tuning 손실 함수(finetuning_loss): 1.495971\n",
      "반복(Epoch): 122, Fine-tuning 손실 함수(finetuning_loss): 1.507283\n",
      "반복(Epoch): 123, Fine-tuning 손실 함수(finetuning_loss): 1.521082\n",
      "반복(Epoch): 124, Fine-tuning 손실 함수(finetuning_loss): 1.513663\n",
      "반복(Epoch): 125, Fine-tuning 손실 함수(finetuning_loss): 1.514000\n",
      "반복(Epoch): 126, Fine-tuning 손실 함수(finetuning_loss): 1.491910\n",
      "반복(Epoch): 127, Fine-tuning 손실 함수(finetuning_loss): 1.494067\n",
      "반복(Epoch): 128, Fine-tuning 손실 함수(finetuning_loss): 1.506169\n",
      "반복(Epoch): 129, Fine-tuning 손실 함수(finetuning_loss): 1.516075\n",
      "반복(Epoch): 130, Fine-tuning 손실 함수(finetuning_loss): 1.485441\n",
      "반복(Epoch): 131, Fine-tuning 손실 함수(finetuning_loss): 1.502506\n",
      "반복(Epoch): 132, Fine-tuning 손실 함수(finetuning_loss): 1.502239\n",
      "반복(Epoch): 133, Fine-tuning 손실 함수(finetuning_loss): 1.489514\n",
      "반복(Epoch): 134, Fine-tuning 손실 함수(finetuning_loss): 1.507318\n",
      "반복(Epoch): 135, Fine-tuning 손실 함수(finetuning_loss): 1.504791\n",
      "반복(Epoch): 136, Fine-tuning 손실 함수(finetuning_loss): 1.503596\n",
      "반복(Epoch): 137, Fine-tuning 손실 함수(finetuning_loss): 1.503005\n",
      "반복(Epoch): 138, Fine-tuning 손실 함수(finetuning_loss): 1.489673\n",
      "반복(Epoch): 139, Fine-tuning 손실 함수(finetuning_loss): 1.506355\n",
      "반복(Epoch): 140, Fine-tuning 손실 함수(finetuning_loss): 1.489468\n",
      "반복(Epoch): 141, Fine-tuning 손실 함수(finetuning_loss): 1.488100\n",
      "반복(Epoch): 142, Fine-tuning 손실 함수(finetuning_loss): 1.507370\n",
      "반복(Epoch): 143, Fine-tuning 손실 함수(finetuning_loss): 1.509819\n",
      "반복(Epoch): 144, Fine-tuning 손실 함수(finetuning_loss): 1.506027\n",
      "반복(Epoch): 145, Fine-tuning 손실 함수(finetuning_loss): 1.499301\n",
      "반복(Epoch): 146, Fine-tuning 손실 함수(finetuning_loss): 1.493517\n",
      "반복(Epoch): 147, Fine-tuning 손실 함수(finetuning_loss): 1.485593\n",
      "반복(Epoch): 148, Fine-tuning 손실 함수(finetuning_loss): 1.491374\n",
      "반복(Epoch): 149, Fine-tuning 손실 함수(finetuning_loss): 1.517126\n",
      "반복(Epoch): 150, Fine-tuning 손실 함수(finetuning_loss): 1.491304\n",
      "반복(Epoch): 151, Fine-tuning 손실 함수(finetuning_loss): 1.477776\n",
      "반복(Epoch): 152, Fine-tuning 손실 함수(finetuning_loss): 1.501370\n",
      "반복(Epoch): 153, Fine-tuning 손실 함수(finetuning_loss): 1.486390\n",
      "반복(Epoch): 154, Fine-tuning 손실 함수(finetuning_loss): 1.495213\n",
      "반복(Epoch): 155, Fine-tuning 손실 함수(finetuning_loss): 1.491305\n",
      "반복(Epoch): 156, Fine-tuning 손실 함수(finetuning_loss): 1.480362\n",
      "반복(Epoch): 157, Fine-tuning 손실 함수(finetuning_loss): 1.507196\n",
      "반복(Epoch): 158, Fine-tuning 손실 함수(finetuning_loss): 1.518255\n",
      "반복(Epoch): 159, Fine-tuning 손실 함수(finetuning_loss): 1.494920\n",
      "반복(Epoch): 160, Fine-tuning 손실 함수(finetuning_loss): 1.501154\n",
      "반복(Epoch): 161, Fine-tuning 손실 함수(finetuning_loss): 1.496388\n",
      "반복(Epoch): 162, Fine-tuning 손실 함수(finetuning_loss): 1.496443\n",
      "반복(Epoch): 163, Fine-tuning 손실 함수(finetuning_loss): 1.511630\n",
      "반복(Epoch): 164, Fine-tuning 손실 함수(finetuning_loss): 1.489905\n",
      "반복(Epoch): 165, Fine-tuning 손실 함수(finetuning_loss): 1.495284\n",
      "반복(Epoch): 166, Fine-tuning 손실 함수(finetuning_loss): 1.488260\n",
      "반복(Epoch): 167, Fine-tuning 손실 함수(finetuning_loss): 1.496453\n",
      "반복(Epoch): 168, Fine-tuning 손실 함수(finetuning_loss): 1.498008\n",
      "반복(Epoch): 169, Fine-tuning 손실 함수(finetuning_loss): 1.487205\n",
      "반복(Epoch): 170, Fine-tuning 손실 함수(finetuning_loss): 1.478981\n",
      "반복(Epoch): 171, Fine-tuning 손실 함수(finetuning_loss): 1.490998\n",
      "반복(Epoch): 172, Fine-tuning 손실 함수(finetuning_loss): 1.484114\n",
      "반복(Epoch): 173, Fine-tuning 손실 함수(finetuning_loss): 1.491837\n",
      "반복(Epoch): 174, Fine-tuning 손실 함수(finetuning_loss): 1.494627\n",
      "반복(Epoch): 175, Fine-tuning 손실 함수(finetuning_loss): 1.493975\n",
      "반복(Epoch): 176, Fine-tuning 손실 함수(finetuning_loss): 1.479799\n",
      "반복(Epoch): 177, Fine-tuning 손실 함수(finetuning_loss): 1.491492\n",
      "반복(Epoch): 178, Fine-tuning 손실 함수(finetuning_loss): 1.502620\n",
      "반복(Epoch): 179, Fine-tuning 손실 함수(finetuning_loss): 1.496462\n",
      "반복(Epoch): 180, Fine-tuning 손실 함수(finetuning_loss): 1.516513\n",
      "반복(Epoch): 181, Fine-tuning 손실 함수(finetuning_loss): 1.503447\n",
      "반복(Epoch): 182, Fine-tuning 손실 함수(finetuning_loss): 1.501193\n",
      "반복(Epoch): 183, Fine-tuning 손실 함수(finetuning_loss): 1.502402\n",
      "반복(Epoch): 184, Fine-tuning 손실 함수(finetuning_loss): 1.503145\n",
      "반복(Epoch): 185, Fine-tuning 손실 함수(finetuning_loss): 1.478281\n",
      "반복(Epoch): 186, Fine-tuning 손실 함수(finetuning_loss): 1.486797\n",
      "반복(Epoch): 187, Fine-tuning 손실 함수(finetuning_loss): 1.501075\n",
      "반복(Epoch): 188, Fine-tuning 손실 함수(finetuning_loss): 1.490756\n",
      "반복(Epoch): 189, Fine-tuning 손실 함수(finetuning_loss): 1.492049\n",
      "반복(Epoch): 190, Fine-tuning 손실 함수(finetuning_loss): 1.497568\n",
      "반복(Epoch): 191, Fine-tuning 손실 함수(finetuning_loss): 1.498067\n",
      "반복(Epoch): 192, Fine-tuning 손실 함수(finetuning_loss): 1.487218\n",
      "반복(Epoch): 193, Fine-tuning 손실 함수(finetuning_loss): 1.483266\n",
      "반복(Epoch): 194, Fine-tuning 손실 함수(finetuning_loss): 1.487932\n",
      "반복(Epoch): 195, Fine-tuning 손실 함수(finetuning_loss): 1.493570\n",
      "반복(Epoch): 196, Fine-tuning 손실 함수(finetuning_loss): 1.485147\n",
      "반복(Epoch): 197, Fine-tuning 손실 함수(finetuning_loss): 1.500882\n",
      "반복(Epoch): 198, Fine-tuning 손실 함수(finetuning_loss): 1.491749\n",
      "반복(Epoch): 199, Fine-tuning 손실 함수(finetuning_loss): 1.486237\n",
      "반복(Epoch): 200, Fine-tuning 손실 함수(finetuning_loss): 1.490359\n",
      "Step 2 : MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화 완료(Fine-Tuning)\n",
      "정확도(오토인코더+Softmax 분류기): 0.956800\n"
     ]
    }
   ],
   "source": [
    "# 세션을 열고 그래프를 실행합니다.\n",
    "with tf.Session() as sess:\n",
    "    # 변수들의 초기값을 할당합니다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 전체 배치 개수를 불러옵니다.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # Step 1: MNIST 데이터 재구축을 위한 오토인코더 최적화(Pre-Training)를 수행합니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        # 모든 배치들에 대해서 최적화를 수행합니다.\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, pretraining_loss_print = sess.run([pretraining_train_step, pretraining_loss], \n",
    "                                                 feed_dict={x: batch_xs})\n",
    "            \n",
    "        # 지정된 epoch마다 학습결과를 출력합니다.\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"반복(Epoch): %d, Pre-Training 손실 함수(pretraining_loss): %f\" % ((epoch+1), \n",
    "                                                                               pretraining_loss_print))\n",
    "            \n",
    "    print(\"Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료(Pre-Training)\")\n",
    "    \n",
    "    # Step 2: MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화(Fine-tuning)를 수행합니다.\n",
    "    for epoch in range(num_epochs + 100):\n",
    "        # 모든 배치들에 대해서 최적화를 수행합니다.\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, finetuning_loss_print = sess.run([finetuning_train_step, finetuning_loss], \n",
    "                                                feed_dict={x: batch_xs,  y: batch_ys})\n",
    "            \n",
    "        # 지정된 epoch마다 학습결과를 출력합니다.\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"반복(Epoch): %d, Fine-tuning 손실 함수(finetuning_loss): %f\" % ((epoch+1), \n",
    "                                                                             finetuning_loss_print))\n",
    "    print(\"Step 2 : MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화 완료(Fine-Tuning)\")\n",
    "    \n",
    "    # 오토인코더+Softmax 분류기 모델의 정확도를 출력합니다.\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred_softmax,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 정확도 : 약 96%\n",
    "    print(\"정확도(오토인코더+Softmax 분류기): %f\" % sess.run(accuracy, \n",
    "            feed_dict={x: mnist.test.images, y: mnist.test.labels})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
