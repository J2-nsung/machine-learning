{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd # data frame\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # visualization\n\nplt.style.use('seaborn')\n\nimport missingno as msno # null data를 보여줌\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/titanic/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for total null values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age, Cabin, Embarked에 null이 존재한다. 이를 채워주어야한다."},{"metadata":{},"cell_type":"markdown","source":"## 생존자는 얼마나 있을까?"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(18, 8))\ndata['Survived'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\n\nsns.countplot('Survived', data=data, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['Sex', 'Survived'])['Survived'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 남,여간 생존률 차이.\n# 여성이 더 많이 생존한 것을 볼 수 있다.\nf, ax = plt.subplots(1, 2, figsize=(15, 8))\ndata[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\n\nsns.countplot('Sex', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Sex: Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.crosstab을 통해 숫자, 색깔로 볼 수 있다.\npd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pclass(1등석, 2등석, 3등석)별 생존률 확인\n# 좋은 좌석일수록 생존률이 높다.\nf, ax = plt.subplots(1, 2, figsize=(16, 8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'], ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\n\nsns.countplot('Pclass', hue='Survived', data=data, ax=ax[1])\nax[1].set_title('Pclass: Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([data.Sex, data.Survived], data.Pclass, margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# factorplot을 통해 꺾은선 그래프 형태로 값을 같이 확인할 수 있다.\nsns.factorplot('Pclass', 'Survived', hue='Sex', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Oldest Passenger was of: {data['Age'].max()} Years\")\nprint(f\"Youngest Passenger was of: {data['Age'].min()} Years\")\nprint(f\"Average Age on the ship: {data['Age'].mean()} Years\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# violinplot을 통해 Pclass, Sex별 생존률을 나이대별로 시각화할 수 있다.\nf, ax = plt.subplots(1, 2, figsize=(18, 8))\nsns.violinplot('Pclass', 'Age', hue='Survived', data=data, scale='count', split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 10))\n\nsns.violinplot('Sex', 'Age', hue='Survived', data=data, scale='count', split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0, 110, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이름 중 'xxx.'인 부분을 추출한다 (ex: Ms. Mrs. Mr. 등등...)\n# 호칭을 추출하는 것으로 생각하면 된다.\ndata['Initial'] = 0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the Initials with the Sex\npd.crosstab(data.Initial, data.Sex).T.style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 위의 결과에 맞게 호칭들을 정리해준다. (Master, Miss, Mr, Mrs, Other)다섯가지만 사용해서 구분할 것이다.\ndata['Initial'].replace(['Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don'],\n                        ['Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Initial')['Age'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null값들은 각 호칭별 나이의 평균으로 넣어준다.\ndata.loc[(data.Age.isnull()) & (data.Initial=='Mr'),'Age'] = 33\ndata.loc[(data.Age.isnull()) & (data.Initial=='Mrs'),'Age'] = 36\ndata.loc[(data.Age.isnull()) & (data.Initial=='Master'),'Age'] = 5\ndata.loc[(data.Age.isnull()) & (data.Initial=='Miss'),'Age'] = 22\ndata.loc[(data.Age.isnull()) & (data.Initial=='Other'),'Age'] = 46","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null값이 채워진 것을 볼 수 있다.\ndata.Age.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 나이대별 생존률을 더 자세히 시각화 해보았다. histogram형식으로 표현.\nf, ax = plt.subplots(1, 2, figsize=(20, 10))\ndata[data['Survived'] == 0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red')\nax[0].set_title('Survived=0')\nx1=list(range(0, 85, 5))\nax[0].set_xticks(x1)\n\ndata[data['Survived'] == 1].Age.plot.hist(ax=ax[1], color='green', bins=20, edgecolor='black')\nax[1].set_title('Survived=1')\nx2=list(range(0, 85, 5))\nax[1].set_xticks(x2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial, Pclass별 생존률을 확인.\n# Mrs, Miss (이상 여성), Master의 생존률이 대체로 높은 것을 볼 수 있다.\n# 역시나 Pclass가 높을 수록 생존률이 높다.\nsns.factorplot('Pclass', 'Survived', col='Initial', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([data.Embarked, data.Pclass], [data.Sex, data.Survived], margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embarked(탑승 항구)별로 생존률 확인\nsns.factorplot('Embarked', 'Survived', data=data)\nf = plt.gcf()\nf.set_size_inches(5, 3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embarked와 다양한 column들과의 연관성을 살펴보았다.\n# 탑승항구별 탑승자의 수, 남,여 성비, 생존률, Pclass\nf, ax = plt.subplots(2, 2, figsize=(20, 15))\nsns.countplot('Embarked', data=data, ax=ax[0, 0])\nax[0, 0].set_title('No. Of Passengers Boarded')\n\nsns.countplot('Embarked', hue='Sex', data=data, ax=ax[0, 1])\nax[0, 1].set_title('Male-Female Split for Embarked')\n\nsns.countplot('Embarked', hue='Survived', data=data, ax=ax[1, 0])\nax[1, 0].set_title('Embarked vs Survived')\n\nsns.countplot('Embarked', hue='Pclass', data=data, ax=ax[1, 1])\nax[1, 1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 탑승항구마다 Pclass별 생존률 확인.\nsns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embarked는 위에서 봤을 때 빈칸이 2개밖에 없으므로 그냥 가장 인원이 많은 S로 채워주었다.\ndata['Embarked'].fillna('S', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Embarked.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([data.SibSp], data.Survived).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 형제자매+아내의 수에따른 생존률을 확인해보았다.\n# 그래프가 왜 이렇게 나오는지 몰겠...\nf, ax = plt.subplots(1, 2, figsize=(20, 8))\nsns.barplot('SibSp', 'Survived', data=data, ax=ax[0])\nax[0].set_title('SibSp vs Survived')\n\nsns.factorplot('SibSp', 'Survived', data=data, ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.SibSp, data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Parch"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 부모, 자녀의 수에 따른 생존률 확인.\nf, ax = plt.subplots(1, 2, figsize=(20, 8))\nsns.barplot(\"Parch\", \"Survived\", data=data, ax=ax[0])\nax[0].set_title('1 Parch vs Survived')\n\nsns.factorplot(\"Parch\", \"Survived\", data=data, ax=ax[1])\nax[1].set_title('2 Parch vs Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Highest Fare was:', data['Fare'].max())\nprint('Lowest Fare was:', data['Fare'].min())\nprint('Averge Fare was:', data['Fare'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pclass마다 요금에 따른 분포를 확인해보았다.\nf, ax = plt.subplots(1, 3, figsize=(20, 8))\nsns.distplot(data[data['Pclass'] == 1].Fare, ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\n\nsns.distplot(data[data['Pclass'] == 2].Fare, ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\n\nsns.distplot(data[data['Pclass'] == 3].Fare, ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 column간 상관관계를 heatmap으로 표현하면 다음과 같다.\n# 데이터가 int형인 것들만 표현가능하다. 즉, str로 되어있는 데이터를 나중에 int로 변환해야함을 뜻한다.\nsns.heatmap(data.corr(), annot=True, cmap='RdYlGn_r', linewidths=0.2)\nf = plt.gcf()\nf.set_size_inches(10, 8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 나이 데이터를 모두 사용하기엔 나이대가 너무 다양하기 때문에 그룹을 지어서 5개로 나누어준다.\ndata['Age_band'] = 0\ndata.loc[data['Age'] <= 16, 'Age_band'] = 0\ndata.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age_band'] = 1\ndata.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age_band'] = 2\ndata.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age_band'] = 3\ndata.loc[data['Age'] > 64, 'Age_band'] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 어린 사람이 많은 것을 알 수 있다.\ndata['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모든 클래스에서 나이가 어릴수록 생존률이 높은 것을 알 수 있다.\nsns.factorplot('Age_band', 'Survived', col='Pclass', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Family_Size별 생존률 확인\ndata['Family_Size'] = 0\ndata['Family_Size'] = data['Parch'] + data['SibSp']\n\ndata['Alone'] = 0\ndata.loc[data.Family_Size == 0, 'Alone'] = 1 # Family_Size가 0이면 alone.\n\nf, ax = plt.subplots(1, 2, figsize=(18, 6))\nsns.factorplot('Family_Size', 'Survived', data=data, ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\n\nsns.factorplot('Alone', 'Survived', data=data, ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pclass별로 혼자인 사람의 성비를 확인한다.\nsns.factorplot('Alone', 'Survived', col='Pclass', hue='Sex', data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Fare_Range'] = pd.qcut(data['Fare'], 4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 요금 데이터도 모두 사용하기엔 너무 다양하기 때문에 4개의 그룹으로 모아준다.\ndata['Fare_cat'] = 0\ndata.loc[data['Fare'] <= 7.91, 'Fare_cat'] = 0\ndata.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare_cat'] = 1\ndata.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31.0), 'Fare_cat'] = 2\ndata.loc[(data['Fare'] > 31.0) & (data['Fare'] <= 512.329), 'Fare_cat'] = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 성별에 따른 요금별 생존률 확인.\nsns.factorplot('Fare_cat', 'Survived', data=data, hue='Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 위에서 말한 str데이터를 int로 변환해준다.\ndata['Sex'].replace(['male', 'female'], [0, 1], inplace=True)\ndata['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)\ndata['Initial'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], [0, 1, 2, 3, 4], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Name', 'Age', 'Ticket', 'Fare', 'Cabin', 'Fare_Range', 'PassengerId'], axis=1, inplace=True)\nsns.heatmap(data.corr(), annot=True, cmap='RdYlGn_r', linewidths=0.2, annot_kws={'size':20})\nf = plt.gcf()\nf.set_size_inches(18, 15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictive Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier # KNN\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split # train, test data split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(data, test_size=0.3, random_state=0, stratify=data['Survived'])\ntrain_X = train[train.columns[1:]]\ntrain_Y = train[train.columns[:1]]\ntest_X = test[test.columns[1:]]\ntest_Y = test[test.columns[:1]]\nX = data[data.columns[1:]]\nY = data['Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Radial Support Vector Machines(rbf-SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(kernel='rbf', C=1, gamma=0.1)\nmodel.fit(train_X, train_Y) # train_X를 train_Y에 근사하도록 학습시킨다.\nprediction1 = model.predict(test_X) # test_X에 대해 정답(test_Y)을 예측한다.\nprint(f'Accuracy of the rbf SVM is {100*metrics.accuracy_score(prediction1, test_Y):.2f}%' ) # 예측값이 실제 정답 test_Y를 얼마나 잘 예측했는지 정확도 측정","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Support Vector Machine (linear-SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(kernel='linear', C=0.1, gamma=0.1)\nmodel.fit(train_X, train_Y)\nprediction2 = model.predict(test_X)\nprint(f'Accuracy of the linear SVM is {100*metrics.accuracy_score(prediction2, test_Y):.2f}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X, train_Y)\nprediction3 = model.predict(test_X)\nprint(f'Accuracy of the Logistic Regression is {100*metrics.accuracy_score(prediction3, test_Y):.2f}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(train_X, train_Y)\nprediction4 = model.predict(test_X)\nprint(f'Accuracy of the Decision Tree is {100*metrics.accuracy_score(prediction4, test_Y):.2f}%' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-Nearest Neighbors(KNN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=9) # 아래 그림을 보면 9일 때가 가장 정확도가 높다.\nmodel.fit(train_X, train_Y)\nprediction5 = model.predict(test_X)\nprint(f'Accuracy of the KNN is {100*metrics.accuracy_score(prediction5, test_Y):.2f}%' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a_index = list(range(1, 11))\na = pd.Series()\nx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nfor i in list(range(1, 11)):\n    model = KNeighborsClassifier(n_neighbors=i)\n    model.fit(train_X, train_Y)\n    prediction = model.predict(test_X)\n    a = a.append(pd.Series(metrics.accuracy_score(prediction, test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nf = plt.gcf()\nf.set_size_inches(12, 6)\nplt.show()\nprint(f'Accuracies for different values of n are: {a.values}, with the max value as {a.values.max()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GaussianNB()\nmodel.fit(train_X, train_Y)\nprediction6=model.predict(test_X)\nprint(f'Accuracy of the Naive Bayes is {100*metrics.accuracy_score(prediction6, test_Y):.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(train_X, train_Y)\nprediction7=model.predict(test_X)\nprint(f'Accuracy of the RandomForest is {100*metrics.accuracy_score(prediction7, test_Y):.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation\n이제 정확도를 올려보자."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델들의 정확도 평균이 약 80%로 근사한다. 오르긴 오르는 건가..?\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score # score evaluation\nfrom sklearn.model_selection import cross_val_predict # prediction\nkfold = KFold(n_splits=10, random_state=22, shuffle=True) # k=10, split the data into 10 equal parts\nxyz = []\naccuracy = []\nstd = []\nclassifiers = ['Linear SVM', 'Radial SVM', 'Logistic Regression', 'KNN', 'Decision Tree', 'Naive Bayes', 'Random Forest']\nmodels = [svm.SVC(kernel='linear'), svm.SVC(kernel='rbf'), LogisticRegression(), KNeighborsClassifier(n_neighbors=9), DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n#     cv_result = cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz, 'Std':std}, index=classifiers)\nnew_models_dataframe2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 교차검증을 하기 전 모델들의 정확도는 아래와 같다.\nplt.subplots(figsize=(12, 6))\nbox = pd.DataFrame(accuracy, index=[classifiers])\nbox.T.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nf = plt.gcf()\nf.set_size_inches(8, 5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confution matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, 3, figsize=(12, 10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0, 0], annot=True, fmt='2.0f')\nax[0, 0].set_title('Matrix for rbf-SVM')\n\ny_pred = cross_val_predict(svm.SVC(kernel='linear'), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0, 1], annot=True, fmt='2.0f')\nax[0, 1].set_title('Matrix for linear-SVM')\n\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0, 2], annot=True, fmt='2.0f')\nax[0, 2].set_title('Matrix for KNN')\n\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1, 0], annot=True, fmt='2.0f')\nax[1, 0].set_title('Matrix for Random-Forests')\n\ny_pred = cross_val_predict(LogisticRegression(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1, 1], annot=True, fmt='2.0f')\nax[1, 1].set_title('Matrix for Logistic Regression')\n\ny_pred = cross_val_predict(DecisionTreeClassifier(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1, 2], annot=True, fmt='2.0f')\nax[1, 2].set_title('Matrix for Decision Tree')\n\ny_pred = cross_val_predict(GaussianNB(), X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax=ax[2, 0], annot=True, fmt='2.0f')\nax[2, 0].set_title('Matrix for Naive Bayes')\n\nplt.subplots_adjust(hspace=0.2, wspace=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (x, y)라고 할 때, (0, 0), (1, 1)은 잘 예측한 것.\n#### (0, 1)은 살아 있는 사람인데 죽은 것으로 예측(95), (1, 0)은 죽은 사람인데 살아 있다고 예측(58)"},{"metadata":{},"cell_type":"markdown","source":"### Hyper-Parameters Tuning\nSVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# C, gamma값을 다양하게 주고 rbf, linear SVM에서 가장 정확도가 높은 모델을 찾는다.\nfrom sklearn.model_selection import GridSearchCV\nC = [0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\ngamma = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nkernel = ['rbf', 'linear']\nhyper = {'kernel': kernel, 'C': C, 'gamma': gamma}\ngd = GridSearchCV(estimator=svm.SVC(), param_grid=hyper, verbose=True)\ngd.fit(X, Y)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"C=0.6, gamma=0.1, kernel='rbf'일 때가 정확도가 약 82.83%로 가장 높다."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = range(100, 1000, 100)\nhyper={'n_estimators': n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0), param_grid=hyper, verbose=True)\ngd.fit(X, Y)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"n_estimators=900일 때, 정확도가 약 81.93%로 가장 높다."},{"metadata":{},"cell_type":"markdown","source":"## Ensembling"},{"metadata":{},"cell_type":"markdown","source":"### Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF', svm.SVC(probability=True, kernel='rbf', C=0.6, gamma=0.1)),\n                                              ('RFor', RandomForestClassifier(n_estimators=900, random_state=0)),\n                                              ('LR', LogisticRegression(C=0.05)),\n                                              ('DT', DecisionTreeClassifier(random_state=0)),\n                                              ('NB', GaussianNB()),\n                                              ('svm', svm.SVC(kernel='linear', probability=True))\n                                             ], voting='soft').fit(train_X, train_Y) # train set에 대해 ensemble\nprint(f'The accuracy for ensembled model is: {100*ensemble_lin_rbf.score(test_X, test_Y):.2f}%') # test set에 대한 정확도\ncross = cross_val_score(ensemble_lin_rbf, X, Y, cv=10, scoring='accuracy')\nprint(f'The cross validated score is {100*cross.mean():.2f}%') # ensembled model에 대해서도 교차 검증을 해준다.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bagging"},{"metadata":{},"cell_type":"markdown","source":"KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3), random_state=0, n_estimators=700)\nmodel.fit(train_X, train_Y)\nprediction = model.predict(test_X)\nprint(f'The accuracy for bagged KNN is {100*metrics.accuracy_score(prediction, test_Y):.2f}%')\nresult = cross_val_score(model, X, Y, cv=10, scoring='accuracy')\nprint(f'The cross validated score for bagged KNN is: {100*result.mean()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=0, n_estimators=100)\nmodel.fit(train_X, train_Y)\nprediction = model.predict(test_X)\nprint(f'The accuracy for bagged Decision Tree is {100*metrics.accuracy_score(prediction, test_Y):.2f}%')\nresult = cross_val_score(model, X, Y, cv=10, scoring='accuracy')\nprint(f'The cross validated score for bagged Decision Tree is: {100*result.mean()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boosting"},{"metadata":{},"cell_type":"markdown","source":"AdaBoost(Adaptive Boosting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.1)\nresult = cross_val_score(ada, X, Y, cv=10, scoring='accuracy')\nprint(f'the cross validated score for AdaBoost is: {100*result.mean():.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SGD Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad = GradientBoostingClassifier(n_estimators=500, random_state=0, learning_rate=0.1)\nresult = cross_val_score(grad, X, Y, cv=10, scoring='accuracy')\nprint(f'The cross validated score for Gradient Boost is: {100*result.mean():.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xg\nxgboost = xg.XGBClassifier(n_estimators=900, learning_rate=0.1)\nresult = cross_val_score(xgboost, X, Y, cv=10, scoring='accuracy')\nprint(f'The cross validated score for XGBoost is: {100*result.mean():.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이 중 가장 높은 AdaBoost를 hyper-parameters tuning"},{"metadata":{},"cell_type":"markdown","source":"#### Hyper-Parameter Tuning for AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = list(range(100, 1100, 100))\nlearn_rate = [0.05, 0.1, 0.2, 0.3, 0.25, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\nhyper = {'n_estimators': n_estimators, 'learning_rate': learn_rate}\ngd = GridSearchCV(estimator=AdaBoostClassifier(), param_grid=hyper, verbose=True)\ngd.fit(X, Y)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"learning_rate = 0.05, n_estimators = 300일 때 정확도가 가장 높다."},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix for the Best Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostClassifier(n_estimators=300, random_state=0, learning_rate=0.05)\nresult = cross_val_predict(ada, X, Y, cv=10)\nsns.heatmap(confusion_matrix(Y, result), annot=True, fmt='2.0f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, 2, figsize=(15, 12))\nmodel = RandomForestClassifier(n_estimators=900, random_state=0)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0, 0])\nax[0, 0].set_title('Feature Importance in Random Forests')\n\nmodel = AdaBoostClassifier(n_estimators=200, learning_rate=0.05, random_state=0)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[0, 1], color='#ddff11')\nax[0, 1].set_title('Feature Importance in AdaBoost')\n\nmodel = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, random_state=0)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1, 0], cmap='RdYlGn_r')\nax[1, 0].set_title('Feature Importance in Gradient Boosting')\n\nmodel = xg.XGBClassifier(n_estimators=900, learning_rate=0.1)\nmodel.fit(X, Y)\npd.Series(model.feature_importances_, X.columns).sort_values(ascending=True).plot.barh(width=0.8, ax=ax[1, 1], color='#FD0F00')\nax[1, 1].set_title('Feature Importance in XGBoost')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}