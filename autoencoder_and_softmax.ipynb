{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 숫자 분류를 위한 Autoencoder+Softmax 분류기 예제 \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-0eea96f49cf9>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터를 다운로드 합니다.\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# 학습에 필요한 설정값들을 정의합니다.\n",
    "learning_rate_RMSProp = 0.02\n",
    "learning_rate_GradientDescent = 0.5\n",
    "num_epochs = 100         # 반복횟수\n",
    "batch_size = 256          \n",
    "display_step = 1         # 몇 Step마다 log를 출력할지 결정합니다.\n",
    "input_size = 784         # MNIST 데이터 input (이미지 크기: 28*28)\n",
    "hidden1_size = 128       # 첫번째 히든레이어의 노드 개수 \n",
    "hidden2_size = 64        # 두번째 히든레이어의 노드 개수 \n",
    "\n",
    "# 입력을 받기 위한 플레이스홀더를 정의합니다.\n",
    "x = tf.placeholder(tf.float32, shape=[None, input_size])   # 인풋을 위한 플레이스홀더를 정의합니다.\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])           # True MNIST 숫자값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder 구조를 정의합니다.\n",
    "def build_autoencoder(x):\n",
    "    # 인코딩(Encoding) - 784 -> 128 -> 64\n",
    "    Wh_1 = tf.Variable(tf.random_normal([input_size, hidden1_size]))   \n",
    "    bh_1 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    H1_output = tf.nn.sigmoid(tf.matmul(x, Wh_1) +bh_1)\n",
    "    \n",
    "    Wh_2 = tf.Variable(tf.random_normal([hidden1_size, hidden2_size]))\n",
    "    bh_2 = tf.Variable(tf.random_normal([hidden2_size]))\n",
    "    H2_output = tf.nn.sigmoid(tf.matmul(H1_output, Wh_2) +bh_2)\n",
    "    \n",
    "    \n",
    "    # 디코딩(Decoding) 64 -> 128 -> 784\n",
    "    Wh_3 = tf.Variable(tf.random_normal([hidden2_size, hidden1_size]))\n",
    "    bh_3 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "    H3_output = tf.nn.sigmoid(tf.matmul(H2_output, Wh_3) +bh_3)\n",
    "    Wo = tf.Variable(tf.random_normal([hidden1_size, input_size]))\n",
    "    bo = tf.Variable(tf.random_normal([input_size]))\n",
    "    X_reconstructed = tf.nn.sigmoid(tf.matmul(H3_output,Wo) + bo)\n",
    "    \n",
    "    return X_reconstructed, H2_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 분류기를 정의합니다.\n",
    "def build_softmax_classifier(x):\n",
    "    # 원본 MNIST 이미지(784) 대신 오토인코더의 압축된 특징(64)을 입력값으로 받습니다.\n",
    "    W_softmax = tf.Variable(tf.zeros([hidden2_size, 10]))    \n",
    "    b_softmax = tf.Variable(tf.zeros([10]))\n",
    "    y_pred = tf.nn.softmax(tf.matmul(x, W_softmax) + b_softmax)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Autoencoder를 선언합니다.\n",
    "# Autoencoder의 Reconstruction 결과(784), 압축된 Features(64)\n",
    "y_pred, extracted_features = build_autoencoder(x) \n",
    "\n",
    "# 타겟데이터는 인풋데이터와 같습니다.\n",
    "y_true = x\n",
    "\n",
    "# Softmax 분류기를 선언합니다. (입력으로 Autoencoder의 압축된 특징을 넣습니다.)\n",
    "y_pred_softmax = build_softmax_classifier(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Jinsung\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre-Training : MNIST 데이터 재구축을 목적으로하는 손실함수와 \n",
    "# 옵티마이저를 정의합니다.\n",
    "pretraining_loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # MSE 손실 함수\n",
    "opt1 = tf.train.RMSPropOptimizer(learning_rate_RMSProp)\n",
    "pretraining_train_step = opt1.minimize(pretraining_loss)\n",
    "\n",
    "\n",
    "# 2. Fine-Tuning :  MNIST 데이터 분류를 목적으로하는 손실함수와 옵티마이저를 \n",
    "# 정의합니다.\n",
    "# cross-entropy loss 함수\n",
    "#finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred_softmax), reduction_indices=[1]))     \n",
    "finetuning_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred_softmax, labels=y))\n",
    "opt2 = tf.train.GradientDescentOptimizer(learning_rate_GradientDescent)\n",
    "finetuning_train_step = opt2.minimize(finetuning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 1, Pre-Training 손실 함수(pretraining_loss): 0.167726\n",
      "반복(Epoch): 2, Pre-Training 손실 함수(pretraining_loss): 0.108654\n",
      "반복(Epoch): 3, Pre-Training 손실 함수(pretraining_loss): 0.093174\n",
      "반복(Epoch): 4, Pre-Training 손실 함수(pretraining_loss): 0.081782\n",
      "반복(Epoch): 5, Pre-Training 손실 함수(pretraining_loss): 0.076231\n",
      "반복(Epoch): 6, Pre-Training 손실 함수(pretraining_loss): 0.070690\n",
      "반복(Epoch): 7, Pre-Training 손실 함수(pretraining_loss): 0.065634\n",
      "반복(Epoch): 8, Pre-Training 손실 함수(pretraining_loss): 0.062544\n",
      "반복(Epoch): 9, Pre-Training 손실 함수(pretraining_loss): 0.061222\n",
      "반복(Epoch): 10, Pre-Training 손실 함수(pretraining_loss): 0.061501\n",
      "반복(Epoch): 11, Pre-Training 손실 함수(pretraining_loss): 0.057205\n",
      "반복(Epoch): 12, Pre-Training 손실 함수(pretraining_loss): 0.056972\n",
      "반복(Epoch): 13, Pre-Training 손실 함수(pretraining_loss): 0.054356\n",
      "반복(Epoch): 14, Pre-Training 손실 함수(pretraining_loss): 0.054157\n",
      "반복(Epoch): 15, Pre-Training 손실 함수(pretraining_loss): 0.055403\n",
      "반복(Epoch): 16, Pre-Training 손실 함수(pretraining_loss): 0.051749\n",
      "반복(Epoch): 17, Pre-Training 손실 함수(pretraining_loss): 0.051010\n",
      "반복(Epoch): 18, Pre-Training 손실 함수(pretraining_loss): 0.049389\n",
      "반복(Epoch): 19, Pre-Training 손실 함수(pretraining_loss): 0.048325\n",
      "반복(Epoch): 20, Pre-Training 손실 함수(pretraining_loss): 0.048929\n",
      "반복(Epoch): 21, Pre-Training 손실 함수(pretraining_loss): 0.045849\n",
      "반복(Epoch): 22, Pre-Training 손실 함수(pretraining_loss): 0.045402\n",
      "반복(Epoch): 23, Pre-Training 손실 함수(pretraining_loss): 0.043560\n",
      "반복(Epoch): 24, Pre-Training 손실 함수(pretraining_loss): 0.043175\n",
      "반복(Epoch): 25, Pre-Training 손실 함수(pretraining_loss): 0.044379\n",
      "반복(Epoch): 26, Pre-Training 손실 함수(pretraining_loss): 0.041523\n",
      "반복(Epoch): 27, Pre-Training 손실 함수(pretraining_loss): 0.042405\n",
      "반복(Epoch): 28, Pre-Training 손실 함수(pretraining_loss): 0.039349\n",
      "반복(Epoch): 29, Pre-Training 손실 함수(pretraining_loss): 0.039497\n",
      "반복(Epoch): 30, Pre-Training 손실 함수(pretraining_loss): 0.038744\n",
      "반복(Epoch): 31, Pre-Training 손실 함수(pretraining_loss): 0.038444\n",
      "반복(Epoch): 32, Pre-Training 손실 함수(pretraining_loss): 0.038939\n",
      "반복(Epoch): 33, Pre-Training 손실 함수(pretraining_loss): 0.035615\n",
      "반복(Epoch): 34, Pre-Training 손실 함수(pretraining_loss): 0.036455\n",
      "반복(Epoch): 35, Pre-Training 손실 함수(pretraining_loss): 0.035543\n",
      "반복(Epoch): 36, Pre-Training 손실 함수(pretraining_loss): 0.032160\n",
      "반복(Epoch): 37, Pre-Training 손실 함수(pretraining_loss): 0.033641\n",
      "반복(Epoch): 38, Pre-Training 손실 함수(pretraining_loss): 0.033313\n",
      "반복(Epoch): 39, Pre-Training 손실 함수(pretraining_loss): 0.032956\n",
      "반복(Epoch): 40, Pre-Training 손실 함수(pretraining_loss): 0.033174\n",
      "반복(Epoch): 41, Pre-Training 손실 함수(pretraining_loss): 0.031747\n",
      "반복(Epoch): 42, Pre-Training 손실 함수(pretraining_loss): 0.029882\n",
      "반복(Epoch): 43, Pre-Training 손실 함수(pretraining_loss): 0.030079\n",
      "반복(Epoch): 44, Pre-Training 손실 함수(pretraining_loss): 0.029825\n",
      "반복(Epoch): 45, Pre-Training 손실 함수(pretraining_loss): 0.031372\n",
      "반복(Epoch): 46, Pre-Training 손실 함수(pretraining_loss): 0.028871\n",
      "반복(Epoch): 47, Pre-Training 손실 함수(pretraining_loss): 0.029138\n",
      "반복(Epoch): 48, Pre-Training 손실 함수(pretraining_loss): 0.029211\n",
      "반복(Epoch): 49, Pre-Training 손실 함수(pretraining_loss): 0.026836\n",
      "반복(Epoch): 50, Pre-Training 손실 함수(pretraining_loss): 0.029080\n",
      "반복(Epoch): 51, Pre-Training 손실 함수(pretraining_loss): 0.027432\n",
      "반복(Epoch): 52, Pre-Training 손실 함수(pretraining_loss): 0.026575\n",
      "반복(Epoch): 53, Pre-Training 손실 함수(pretraining_loss): 0.028630\n",
      "반복(Epoch): 54, Pre-Training 손실 함수(pretraining_loss): 0.027519\n",
      "반복(Epoch): 55, Pre-Training 손실 함수(pretraining_loss): 0.027123\n",
      "반복(Epoch): 56, Pre-Training 손실 함수(pretraining_loss): 0.025625\n",
      "반복(Epoch): 57, Pre-Training 손실 함수(pretraining_loss): 0.027437\n",
      "반복(Epoch): 58, Pre-Training 손실 함수(pretraining_loss): 0.026518\n",
      "반복(Epoch): 59, Pre-Training 손실 함수(pretraining_loss): 0.027088\n",
      "반복(Epoch): 60, Pre-Training 손실 함수(pretraining_loss): 0.025654\n",
      "반복(Epoch): 61, Pre-Training 손실 함수(pretraining_loss): 0.026471\n",
      "반복(Epoch): 62, Pre-Training 손실 함수(pretraining_loss): 0.027211\n",
      "반복(Epoch): 63, Pre-Training 손실 함수(pretraining_loss): 0.026181\n",
      "반복(Epoch): 64, Pre-Training 손실 함수(pretraining_loss): 0.024369\n",
      "반복(Epoch): 65, Pre-Training 손실 함수(pretraining_loss): 0.025462\n",
      "반복(Epoch): 66, Pre-Training 손실 함수(pretraining_loss): 0.026049\n",
      "반복(Epoch): 67, Pre-Training 손실 함수(pretraining_loss): 0.026745\n",
      "반복(Epoch): 68, Pre-Training 손실 함수(pretraining_loss): 0.023739\n",
      "반복(Epoch): 69, Pre-Training 손실 함수(pretraining_loss): 0.024103\n",
      "반복(Epoch): 70, Pre-Training 손실 함수(pretraining_loss): 0.025056\n",
      "반복(Epoch): 71, Pre-Training 손실 함수(pretraining_loss): 0.025047\n",
      "반복(Epoch): 72, Pre-Training 손실 함수(pretraining_loss): 0.025758\n",
      "반복(Epoch): 73, Pre-Training 손실 함수(pretraining_loss): 0.024952\n",
      "반복(Epoch): 74, Pre-Training 손실 함수(pretraining_loss): 0.023319\n",
      "반복(Epoch): 75, Pre-Training 손실 함수(pretraining_loss): 0.023594\n",
      "반복(Epoch): 76, Pre-Training 손실 함수(pretraining_loss): 0.022808\n",
      "반복(Epoch): 77, Pre-Training 손실 함수(pretraining_loss): 0.022972\n",
      "반복(Epoch): 78, Pre-Training 손실 함수(pretraining_loss): 0.020622\n",
      "반복(Epoch): 79, Pre-Training 손실 함수(pretraining_loss): 0.020077\n",
      "반복(Epoch): 80, Pre-Training 손실 함수(pretraining_loss): 0.021902\n",
      "반복(Epoch): 81, Pre-Training 손실 함수(pretraining_loss): 0.021847\n",
      "반복(Epoch): 82, Pre-Training 손실 함수(pretraining_loss): 0.021381\n",
      "반복(Epoch): 83, Pre-Training 손실 함수(pretraining_loss): 0.021150\n",
      "반복(Epoch): 84, Pre-Training 손실 함수(pretraining_loss): 0.022099\n",
      "반복(Epoch): 85, Pre-Training 손실 함수(pretraining_loss): 0.021099\n",
      "반복(Epoch): 86, Pre-Training 손실 함수(pretraining_loss): 0.020161\n",
      "반복(Epoch): 87, Pre-Training 손실 함수(pretraining_loss): 0.018069\n",
      "반복(Epoch): 88, Pre-Training 손실 함수(pretraining_loss): 0.018559\n",
      "반복(Epoch): 89, Pre-Training 손실 함수(pretraining_loss): 0.019432\n",
      "반복(Epoch): 90, Pre-Training 손실 함수(pretraining_loss): 0.019583\n",
      "반복(Epoch): 91, Pre-Training 손실 함수(pretraining_loss): 0.018300\n",
      "반복(Epoch): 92, Pre-Training 손실 함수(pretraining_loss): 0.017329\n",
      "반복(Epoch): 93, Pre-Training 손실 함수(pretraining_loss): 0.018787\n",
      "반복(Epoch): 94, Pre-Training 손실 함수(pretraining_loss): 0.017897\n",
      "반복(Epoch): 95, Pre-Training 손실 함수(pretraining_loss): 0.017617\n",
      "반복(Epoch): 96, Pre-Training 손실 함수(pretraining_loss): 0.017776\n",
      "반복(Epoch): 97, Pre-Training 손실 함수(pretraining_loss): 0.018063\n",
      "반복(Epoch): 98, Pre-Training 손실 함수(pretraining_loss): 0.017372\n",
      "반복(Epoch): 99, Pre-Training 손실 함수(pretraining_loss): 0.018664\n",
      "반복(Epoch): 100, Pre-Training 손실 함수(pretraining_loss): 0.017429\n",
      "Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료(Pre-Training)\n",
      "반복(Epoch): 1, Fine-tuning 손실 함수(finetuning_loss): 1.946140\n",
      "반복(Epoch): 2, Fine-tuning 손실 함수(finetuning_loss): 1.814001\n",
      "반복(Epoch): 3, Fine-tuning 손실 함수(finetuning_loss): 1.712747\n",
      "반복(Epoch): 4, Fine-tuning 손실 함수(finetuning_loss): 1.712901\n",
      "반복(Epoch): 5, Fine-tuning 손실 함수(finetuning_loss): 1.701185\n",
      "반복(Epoch): 6, Fine-tuning 손실 함수(finetuning_loss): 1.666519\n",
      "반복(Epoch): 7, Fine-tuning 손실 함수(finetuning_loss): 1.685795\n",
      "반복(Epoch): 8, Fine-tuning 손실 함수(finetuning_loss): 1.685322\n",
      "반복(Epoch): 9, Fine-tuning 손실 함수(finetuning_loss): 1.673708\n",
      "반복(Epoch): 10, Fine-tuning 손실 함수(finetuning_loss): 1.671823\n",
      "반복(Epoch): 11, Fine-tuning 손실 함수(finetuning_loss): 1.647852\n",
      "반복(Epoch): 12, Fine-tuning 손실 함수(finetuning_loss): 1.604870\n",
      "반복(Epoch): 13, Fine-tuning 손실 함수(finetuning_loss): 1.650134\n",
      "반복(Epoch): 14, Fine-tuning 손실 함수(finetuning_loss): 1.628874\n",
      "반복(Epoch): 15, Fine-tuning 손실 함수(finetuning_loss): 1.626771\n",
      "반복(Epoch): 16, Fine-tuning 손실 함수(finetuning_loss): 1.587312\n",
      "반복(Epoch): 17, Fine-tuning 손실 함수(finetuning_loss): 1.598506\n",
      "반복(Epoch): 18, Fine-tuning 손실 함수(finetuning_loss): 1.561885\n",
      "반복(Epoch): 19, Fine-tuning 손실 함수(finetuning_loss): 1.593679\n",
      "반복(Epoch): 20, Fine-tuning 손실 함수(finetuning_loss): 1.578186\n",
      "반복(Epoch): 21, Fine-tuning 손실 함수(finetuning_loss): 1.553116\n",
      "반복(Epoch): 22, Fine-tuning 손실 함수(finetuning_loss): 1.558249\n",
      "반복(Epoch): 23, Fine-tuning 손실 함수(finetuning_loss): 1.548107\n",
      "반복(Epoch): 24, Fine-tuning 손실 함수(finetuning_loss): 1.551558\n",
      "반복(Epoch): 25, Fine-tuning 손실 함수(finetuning_loss): 1.551608\n",
      "반복(Epoch): 26, Fine-tuning 손실 함수(finetuning_loss): 1.565367\n",
      "반복(Epoch): 27, Fine-tuning 손실 함수(finetuning_loss): 1.523943\n",
      "반복(Epoch): 28, Fine-tuning 손실 함수(finetuning_loss): 1.521023\n",
      "반복(Epoch): 29, Fine-tuning 손실 함수(finetuning_loss): 1.545152\n",
      "반복(Epoch): 30, Fine-tuning 손실 함수(finetuning_loss): 1.548272\n",
      "반복(Epoch): 31, Fine-tuning 손실 함수(finetuning_loss): 1.540876\n",
      "반복(Epoch): 32, Fine-tuning 손실 함수(finetuning_loss): 1.526659\n",
      "반복(Epoch): 33, Fine-tuning 손실 함수(finetuning_loss): 1.542892\n",
      "반복(Epoch): 34, Fine-tuning 손실 함수(finetuning_loss): 1.550732\n",
      "반복(Epoch): 35, Fine-tuning 손실 함수(finetuning_loss): 1.533410\n",
      "반복(Epoch): 36, Fine-tuning 손실 함수(finetuning_loss): 1.536387\n",
      "반복(Epoch): 37, Fine-tuning 손실 함수(finetuning_loss): 1.526401\n",
      "반복(Epoch): 38, Fine-tuning 손실 함수(finetuning_loss): 1.534006\n",
      "반복(Epoch): 39, Fine-tuning 손실 함수(finetuning_loss): 1.525637\n",
      "반복(Epoch): 40, Fine-tuning 손실 함수(finetuning_loss): 1.512826\n",
      "반복(Epoch): 41, Fine-tuning 손실 함수(finetuning_loss): 1.538033\n",
      "반복(Epoch): 42, Fine-tuning 손실 함수(finetuning_loss): 1.537078\n",
      "반복(Epoch): 43, Fine-tuning 손실 함수(finetuning_loss): 1.531794\n",
      "반복(Epoch): 44, Fine-tuning 손실 함수(finetuning_loss): 1.516944\n",
      "반복(Epoch): 45, Fine-tuning 손실 함수(finetuning_loss): 1.518533\n",
      "반복(Epoch): 46, Fine-tuning 손실 함수(finetuning_loss): 1.519351\n",
      "반복(Epoch): 47, Fine-tuning 손실 함수(finetuning_loss): 1.506088\n",
      "반복(Epoch): 48, Fine-tuning 손실 함수(finetuning_loss): 1.506881\n",
      "반복(Epoch): 49, Fine-tuning 손실 함수(finetuning_loss): 1.537884\n",
      "반복(Epoch): 50, Fine-tuning 손실 함수(finetuning_loss): 1.524185\n",
      "반복(Epoch): 51, Fine-tuning 손실 함수(finetuning_loss): 1.538404\n",
      "반복(Epoch): 52, Fine-tuning 손실 함수(finetuning_loss): 1.531857\n",
      "반복(Epoch): 53, Fine-tuning 손실 함수(finetuning_loss): 1.520750\n",
      "반복(Epoch): 54, Fine-tuning 손실 함수(finetuning_loss): 1.530494\n",
      "반복(Epoch): 55, Fine-tuning 손실 함수(finetuning_loss): 1.530105\n",
      "반복(Epoch): 56, Fine-tuning 손실 함수(finetuning_loss): 1.527520\n",
      "반복(Epoch): 57, Fine-tuning 손실 함수(finetuning_loss): 1.522068\n",
      "반복(Epoch): 58, Fine-tuning 손실 함수(finetuning_loss): 1.518639\n",
      "반복(Epoch): 59, Fine-tuning 손실 함수(finetuning_loss): 1.527094\n",
      "반복(Epoch): 60, Fine-tuning 손실 함수(finetuning_loss): 1.511001\n",
      "반복(Epoch): 61, Fine-tuning 손실 함수(finetuning_loss): 1.501077\n",
      "반복(Epoch): 62, Fine-tuning 손실 함수(finetuning_loss): 1.517125\n",
      "반복(Epoch): 63, Fine-tuning 손실 함수(finetuning_loss): 1.511462\n",
      "반복(Epoch): 64, Fine-tuning 손실 함수(finetuning_loss): 1.526213\n",
      "반복(Epoch): 65, Fine-tuning 손실 함수(finetuning_loss): 1.530530\n",
      "반복(Epoch): 66, Fine-tuning 손실 함수(finetuning_loss): 1.502827\n",
      "반복(Epoch): 67, Fine-tuning 손실 함수(finetuning_loss): 1.517565\n",
      "반복(Epoch): 68, Fine-tuning 손실 함수(finetuning_loss): 1.515757\n",
      "반복(Epoch): 69, Fine-tuning 손실 함수(finetuning_loss): 1.530834\n",
      "반복(Epoch): 70, Fine-tuning 손실 함수(finetuning_loss): 1.498906\n",
      "반복(Epoch): 71, Fine-tuning 손실 함수(finetuning_loss): 1.524250\n",
      "반복(Epoch): 72, Fine-tuning 손실 함수(finetuning_loss): 1.511551\n",
      "반복(Epoch): 73, Fine-tuning 손실 함수(finetuning_loss): 1.519077\n",
      "반복(Epoch): 74, Fine-tuning 손실 함수(finetuning_loss): 1.502067\n",
      "반복(Epoch): 75, Fine-tuning 손실 함수(finetuning_loss): 1.531651\n",
      "반복(Epoch): 76, Fine-tuning 손실 함수(finetuning_loss): 1.507039\n",
      "반복(Epoch): 77, Fine-tuning 손실 함수(finetuning_loss): 1.521352\n",
      "반복(Epoch): 78, Fine-tuning 손실 함수(finetuning_loss): 1.490382\n",
      "반복(Epoch): 79, Fine-tuning 손실 함수(finetuning_loss): 1.512844\n",
      "반복(Epoch): 80, Fine-tuning 손실 함수(finetuning_loss): 1.498307\n",
      "반복(Epoch): 81, Fine-tuning 손실 함수(finetuning_loss): 1.529488\n",
      "반복(Epoch): 82, Fine-tuning 손실 함수(finetuning_loss): 1.498431\n",
      "반복(Epoch): 83, Fine-tuning 손실 함수(finetuning_loss): 1.519428\n",
      "반복(Epoch): 84, Fine-tuning 손실 함수(finetuning_loss): 1.517865\n",
      "반복(Epoch): 85, Fine-tuning 손실 함수(finetuning_loss): 1.498779\n",
      "반복(Epoch): 86, Fine-tuning 손실 함수(finetuning_loss): 1.513759\n",
      "반복(Epoch): 87, Fine-tuning 손실 함수(finetuning_loss): 1.519435\n",
      "반복(Epoch): 88, Fine-tuning 손실 함수(finetuning_loss): 1.497711\n",
      "반복(Epoch): 89, Fine-tuning 손실 함수(finetuning_loss): 1.512391\n",
      "반복(Epoch): 90, Fine-tuning 손실 함수(finetuning_loss): 1.516210\n",
      "반복(Epoch): 91, Fine-tuning 손실 함수(finetuning_loss): 1.496402\n",
      "반복(Epoch): 92, Fine-tuning 손실 함수(finetuning_loss): 1.507857\n",
      "반복(Epoch): 93, Fine-tuning 손실 함수(finetuning_loss): 1.522794\n",
      "반복(Epoch): 94, Fine-tuning 손실 함수(finetuning_loss): 1.515905\n",
      "반복(Epoch): 95, Fine-tuning 손실 함수(finetuning_loss): 1.508062\n",
      "반복(Epoch): 96, Fine-tuning 손실 함수(finetuning_loss): 1.509041\n",
      "반복(Epoch): 97, Fine-tuning 손실 함수(finetuning_loss): 1.516810\n",
      "반복(Epoch): 98, Fine-tuning 손실 함수(finetuning_loss): 1.503100\n",
      "반복(Epoch): 99, Fine-tuning 손실 함수(finetuning_loss): 1.503607\n",
      "반복(Epoch): 100, Fine-tuning 손실 함수(finetuning_loss): 1.521086\n",
      "반복(Epoch): 101, Fine-tuning 손실 함수(finetuning_loss): 1.514123\n",
      "반복(Epoch): 102, Fine-tuning 손실 함수(finetuning_loss): 1.488614\n",
      "반복(Epoch): 103, Fine-tuning 손실 함수(finetuning_loss): 1.503942\n",
      "반복(Epoch): 104, Fine-tuning 손실 함수(finetuning_loss): 1.517326\n",
      "반복(Epoch): 105, Fine-tuning 손실 함수(finetuning_loss): 1.505312\n",
      "반복(Epoch): 106, Fine-tuning 손실 함수(finetuning_loss): 1.503774\n",
      "반복(Epoch): 107, Fine-tuning 손실 함수(finetuning_loss): 1.497550\n",
      "반복(Epoch): 108, Fine-tuning 손실 함수(finetuning_loss): 1.498604\n",
      "반복(Epoch): 109, Fine-tuning 손실 함수(finetuning_loss): 1.486718\n",
      "반복(Epoch): 110, Fine-tuning 손실 함수(finetuning_loss): 1.477467\n",
      "반복(Epoch): 111, Fine-tuning 손실 함수(finetuning_loss): 1.499526\n",
      "반복(Epoch): 112, Fine-tuning 손실 함수(finetuning_loss): 1.484019\n",
      "반복(Epoch): 113, Fine-tuning 손실 함수(finetuning_loss): 1.484511\n",
      "반복(Epoch): 114, Fine-tuning 손실 함수(finetuning_loss): 1.502203\n",
      "반복(Epoch): 115, Fine-tuning 손실 함수(finetuning_loss): 1.504757\n",
      "반복(Epoch): 116, Fine-tuning 손실 함수(finetuning_loss): 1.503192\n",
      "반복(Epoch): 117, Fine-tuning 손실 함수(finetuning_loss): 1.489946\n",
      "반복(Epoch): 118, Fine-tuning 손실 함수(finetuning_loss): 1.511229\n",
      "반복(Epoch): 119, Fine-tuning 손실 함수(finetuning_loss): 1.485203\n",
      "반복(Epoch): 120, Fine-tuning 손실 함수(finetuning_loss): 1.509146\n",
      "반복(Epoch): 121, Fine-tuning 손실 함수(finetuning_loss): 1.491655\n",
      "반복(Epoch): 122, Fine-tuning 손실 함수(finetuning_loss): 1.484998\n",
      "반복(Epoch): 123, Fine-tuning 손실 함수(finetuning_loss): 1.530129\n",
      "반복(Epoch): 124, Fine-tuning 손실 함수(finetuning_loss): 1.500100\n",
      "반복(Epoch): 125, Fine-tuning 손실 함수(finetuning_loss): 1.495750\n",
      "반복(Epoch): 126, Fine-tuning 손실 함수(finetuning_loss): 1.487217\n",
      "반복(Epoch): 127, Fine-tuning 손실 함수(finetuning_loss): 1.489893\n",
      "반복(Epoch): 128, Fine-tuning 손실 함수(finetuning_loss): 1.529825\n",
      "반복(Epoch): 129, Fine-tuning 손실 함수(finetuning_loss): 1.493471\n",
      "반복(Epoch): 130, Fine-tuning 손실 함수(finetuning_loss): 1.518286\n",
      "반복(Epoch): 131, Fine-tuning 손실 함수(finetuning_loss): 1.488187\n",
      "반복(Epoch): 132, Fine-tuning 손실 함수(finetuning_loss): 1.493700\n",
      "반복(Epoch): 133, Fine-tuning 손실 함수(finetuning_loss): 1.501683\n",
      "반복(Epoch): 134, Fine-tuning 손실 함수(finetuning_loss): 1.492960\n",
      "반복(Epoch): 135, Fine-tuning 손실 함수(finetuning_loss): 1.510068\n",
      "반복(Epoch): 136, Fine-tuning 손실 함수(finetuning_loss): 1.493833\n",
      "반복(Epoch): 137, Fine-tuning 손실 함수(finetuning_loss): 1.508233\n",
      "반복(Epoch): 138, Fine-tuning 손실 함수(finetuning_loss): 1.500087\n",
      "반복(Epoch): 139, Fine-tuning 손실 함수(finetuning_loss): 1.483572\n",
      "반복(Epoch): 140, Fine-tuning 손실 함수(finetuning_loss): 1.513239\n",
      "반복(Epoch): 141, Fine-tuning 손실 함수(finetuning_loss): 1.506257\n",
      "반복(Epoch): 142, Fine-tuning 손실 함수(finetuning_loss): 1.492095\n",
      "반복(Epoch): 143, Fine-tuning 손실 함수(finetuning_loss): 1.491204\n",
      "반복(Epoch): 144, Fine-tuning 손실 함수(finetuning_loss): 1.497597\n",
      "반복(Epoch): 145, Fine-tuning 손실 함수(finetuning_loss): 1.491273\n",
      "반복(Epoch): 146, Fine-tuning 손실 함수(finetuning_loss): 1.490498\n",
      "반복(Epoch): 147, Fine-tuning 손실 함수(finetuning_loss): 1.494358\n",
      "반복(Epoch): 148, Fine-tuning 손실 함수(finetuning_loss): 1.495520\n",
      "반복(Epoch): 149, Fine-tuning 손실 함수(finetuning_loss): 1.491549\n",
      "반복(Epoch): 150, Fine-tuning 손실 함수(finetuning_loss): 1.492409\n",
      "반복(Epoch): 151, Fine-tuning 손실 함수(finetuning_loss): 1.502315\n",
      "반복(Epoch): 152, Fine-tuning 손실 함수(finetuning_loss): 1.500053\n",
      "반복(Epoch): 153, Fine-tuning 손실 함수(finetuning_loss): 1.486819\n",
      "반복(Epoch): 154, Fine-tuning 손실 함수(finetuning_loss): 1.500332\n",
      "반복(Epoch): 155, Fine-tuning 손실 함수(finetuning_loss): 1.497453\n",
      "반복(Epoch): 156, Fine-tuning 손실 함수(finetuning_loss): 1.494907\n",
      "반복(Epoch): 157, Fine-tuning 손실 함수(finetuning_loss): 1.478734\n",
      "반복(Epoch): 158, Fine-tuning 손실 함수(finetuning_loss): 1.498907\n",
      "반복(Epoch): 159, Fine-tuning 손실 함수(finetuning_loss): 1.509282\n",
      "반복(Epoch): 160, Fine-tuning 손실 함수(finetuning_loss): 1.496711\n",
      "반복(Epoch): 161, Fine-tuning 손실 함수(finetuning_loss): 1.491643\n",
      "반복(Epoch): 162, Fine-tuning 손실 함수(finetuning_loss): 1.488742\n",
      "반복(Epoch): 163, Fine-tuning 손실 함수(finetuning_loss): 1.505105\n",
      "반복(Epoch): 164, Fine-tuning 손실 함수(finetuning_loss): 1.491391\n",
      "반복(Epoch): 165, Fine-tuning 손실 함수(finetuning_loss): 1.498880\n",
      "반복(Epoch): 166, Fine-tuning 손실 함수(finetuning_loss): 1.497144\n",
      "반복(Epoch): 167, Fine-tuning 손실 함수(finetuning_loss): 1.489455\n",
      "반복(Epoch): 168, Fine-tuning 손실 함수(finetuning_loss): 1.487213\n",
      "반복(Epoch): 169, Fine-tuning 손실 함수(finetuning_loss): 1.497292\n",
      "반복(Epoch): 170, Fine-tuning 손실 함수(finetuning_loss): 1.491769\n",
      "반복(Epoch): 171, Fine-tuning 손실 함수(finetuning_loss): 1.482709\n",
      "반복(Epoch): 172, Fine-tuning 손실 함수(finetuning_loss): 1.510296\n",
      "반복(Epoch): 173, Fine-tuning 손실 함수(finetuning_loss): 1.486449\n",
      "반복(Epoch): 174, Fine-tuning 손실 함수(finetuning_loss): 1.499406\n",
      "반복(Epoch): 175, Fine-tuning 손실 함수(finetuning_loss): 1.495866\n",
      "반복(Epoch): 176, Fine-tuning 손실 함수(finetuning_loss): 1.511454\n",
      "반복(Epoch): 177, Fine-tuning 손실 함수(finetuning_loss): 1.502646\n",
      "반복(Epoch): 178, Fine-tuning 손실 함수(finetuning_loss): 1.478956\n",
      "반복(Epoch): 179, Fine-tuning 손실 함수(finetuning_loss): 1.479609\n",
      "반복(Epoch): 180, Fine-tuning 손실 함수(finetuning_loss): 1.497001\n",
      "반복(Epoch): 181, Fine-tuning 손실 함수(finetuning_loss): 1.498787\n",
      "반복(Epoch): 182, Fine-tuning 손실 함수(finetuning_loss): 1.488773\n",
      "반복(Epoch): 183, Fine-tuning 손실 함수(finetuning_loss): 1.494729\n",
      "반복(Epoch): 184, Fine-tuning 손실 함수(finetuning_loss): 1.514874\n",
      "반복(Epoch): 185, Fine-tuning 손실 함수(finetuning_loss): 1.498279\n",
      "반복(Epoch): 186, Fine-tuning 손실 함수(finetuning_loss): 1.501989\n",
      "반복(Epoch): 187, Fine-tuning 손실 함수(finetuning_loss): 1.482276\n",
      "반복(Epoch): 188, Fine-tuning 손실 함수(finetuning_loss): 1.496070\n",
      "반복(Epoch): 189, Fine-tuning 손실 함수(finetuning_loss): 1.489077\n",
      "반복(Epoch): 190, Fine-tuning 손실 함수(finetuning_loss): 1.488607\n",
      "반복(Epoch): 191, Fine-tuning 손실 함수(finetuning_loss): 1.472178\n",
      "반복(Epoch): 192, Fine-tuning 손실 함수(finetuning_loss): 1.478944\n",
      "반복(Epoch): 193, Fine-tuning 손실 함수(finetuning_loss): 1.500835\n",
      "반복(Epoch): 194, Fine-tuning 손실 함수(finetuning_loss): 1.493942\n",
      "반복(Epoch): 195, Fine-tuning 손실 함수(finetuning_loss): 1.502060\n",
      "반복(Epoch): 196, Fine-tuning 손실 함수(finetuning_loss): 1.488924\n",
      "반복(Epoch): 197, Fine-tuning 손실 함수(finetuning_loss): 1.479586\n",
      "반복(Epoch): 198, Fine-tuning 손실 함수(finetuning_loss): 1.486963\n",
      "반복(Epoch): 199, Fine-tuning 손실 함수(finetuning_loss): 1.481221\n",
      "반복(Epoch): 200, Fine-tuning 손실 함수(finetuning_loss): 1.492526\n",
      "Step 2 : MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화 완료(Fine-Tuning)\n",
      "정확도(오토인코더+Softmax 분류기): 0.957800\n"
     ]
    }
   ],
   "source": [
    "# 세션을 열고 그래프를 실행합니다.\n",
    "with tf.Session() as sess:\n",
    "    # 변수들의 초기값을 할당합니다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 전체 배치 개수를 불러옵니다.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # Step 1: MNIST 데이터 재구축을 위한 오토인코더 최적화(Pre-Training)를 수행합니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        # 모든 배치들에 대해서 최적화를 수행합니다.\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, pretraining_loss_print = sess.run([pretraining_train_step, pretraining_loss], \n",
    "                                                 feed_dict={x: batch_xs})\n",
    "            \n",
    "        # 지정된 epoch마다 학습결과를 출력합니다.\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"반복(Epoch): %d, Pre-Training 손실 함수(pretraining_loss): %f\" % ((epoch+1), \n",
    "                                                                               pretraining_loss_print))\n",
    "            \n",
    "    print(\"Step 1 : MNIST 데이터 재구축을 위한 오토인코더 최적화 완료(Pre-Training)\")\n",
    "    \n",
    "    # Step 2: MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화(Fine-tuning)를 수행합니다.\n",
    "    for epoch in range(num_epochs + 100):\n",
    "        # 모든 배치들에 대해서 최적화를 수행합니다.\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, finetuning_loss_print = sess.run([finetuning_train_step, finetuning_loss], \n",
    "                                                feed_dict={x: batch_xs,  y: batch_ys})\n",
    "            \n",
    "        # 지정된 epoch마다 학습결과를 출력합니다.\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"반복(Epoch): %d, Fine-tuning 손실 함수(finetuning_loss): %f\" % ((epoch+1), \n",
    "                                                                             finetuning_loss_print))\n",
    "    print(\"Step 2 : MNIST 데이터 분류를 위한 오토인코더+Softmax 분류기 최적화 완료(Fine-Tuning)\")\n",
    "    \n",
    "    # 오토인코더+Softmax 분류기 모델의 정확도를 출력합니다.\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred_softmax,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 정확도 : 약 96%\n",
    "    print(\"정확도(오토인코더+Softmax 분류기): %f\" % sess.run(accuracy, \n",
    "            feed_dict={x: mnist.test.images, y: mnist.test.labels})) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
